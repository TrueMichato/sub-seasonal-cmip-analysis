{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import netCDF4 as nc\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a X Design matrix made in shape (400*600, 24)\n",
    "when each row is a grid point and each feature is a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def flatten_models_to_grid_matrix(interpolated_precip_list: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a design matrix X where each row represents a grid point, and each column \n",
    "    (feature) represents the flattened precipitation data from a different model.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    interpolated_precip_list : List[np.ndarray]\n",
    "        A list of 2D arrays (matrices) with dimensions (400, 600) representing \n",
    "        precipitation data from different models. Each matrix corresponds to a different \n",
    "        model.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        A 2D array (design matrix X) of shape (240000, num_models), where:\n",
    "        - Each row corresponds to a specific grid point across all models.\n",
    "        - Each column corresponds to the precipitation data for that grid point from \n",
    "          a particular model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of models and number of grid points\n",
    "    num_models = len(interpolated_precip_list)\n",
    "    num_grid_points = 400 * 600\n",
    "\n",
    "    # Initialize an empty matrix to store the reshaped data\n",
    "    X = np.zeros((num_grid_points, num_models))\n",
    "\n",
    "    # Loop over each model's matrix and reshape it into a vector\n",
    "    for i, precip_matrix in enumerate(interpolated_precip_list):\n",
    "        # Reshape each (400, 600) matrix to (240000,) and assign it to the ith column\n",
    "        X[:, i] = precip_matrix.reshape(-1)\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_relative_path = os.path.join('..', 'Data')\n",
    "ERA5_path = os.path.join(data_relative_path,'ERA5')\n",
    "CHIRPS_path = os.path.join(data_relative_path,'CHIRPS2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interpolate to target dimensions\n",
    "def extrarpolate_to_target_dim(data, target_dim):\n",
    "    m, n = data.shape[1:]  # Original dimensions\n",
    "    interpolated_data = np.empty((data.shape[0], target_dim[0], target_dim[1]))  # Create an empty array to hold the results\n",
    "\n",
    "    for i in range(data.shape[0]):  # Iterate over the time dimension\n",
    "        interpolator = scipy.interpolate.RegularGridInterpolator(\n",
    "            (np.linspace(0, 1, m), np.linspace(0, 1, n)), \n",
    "            data[i, :, :], \n",
    "            method='linear'\n",
    "        )\n",
    "        new_x = np.linspace(0, 1, target_dim[0])\n",
    "        new_y = np.linspace(0, 1, target_dim[1])\n",
    "        new_grid_x, new_grid_y = np.meshgrid(new_x, new_y, indexing='ij')\n",
    "        points = np.stack([new_grid_x.ravel(), new_grid_y.ravel()], axis=-1)\n",
    "        interpolated_slice = interpolator(points).reshape(target_dim)\n",
    "        interpolated_data[i, :, :] = interpolated_slice\n",
    "    return interpolated_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSLP_ERA5_October.nc shape for subset (time, lat, lon): (42, 400, 600)\n",
      "MSLP_ERA5_October.nc data for first time step: [[101699.42311507 101706.21204295 101713.00097082 ... 102028.60665948\n",
      "  102028.45460181 102028.30254414]\n",
      " [101693.68485647 101700.47378435 101707.26271223 ... 102026.24735567\n",
      "  102025.95899981 102025.67064394]\n",
      " [101687.94659787 101694.73552575 101701.52445363 ... 102023.88805186\n",
      "  102023.4633978  102023.03874374]\n",
      " ...\n",
      " [101400.26121866 101396.79204851 101393.32287837 ... 101220.3552735\n",
      "  101219.16387484 101217.97247619]\n",
      " [101394.97951417 101391.60360069 101388.2276872  ... 101220.16550218\n",
      "  101218.97051673 101217.77553127]\n",
      " [101389.69780969 101386.41515287 101383.13249604 ... 101219.97573085\n",
      "  101218.77715861 101217.57858636]]\n",
      "GPH_ERA5_October.nc shape for subset (time, lat, lon): (42, 400, 600)\n",
      "GPH_ERA5_October.nc data for first time step: [[4030.22920353 4400.18566087 4770.14211821 ... -292.71020542\n",
      "  -293.78722567 -294.86424593]\n",
      " [3600.31876867 3969.19550645 4338.07224424 ... -286.2033955\n",
      "  -288.28815401 -290.37291251]\n",
      " [3170.40833381 3538.20535204 3906.00237027 ... -279.69658558\n",
      "  -282.78908234 -285.88157909]\n",
      " ...\n",
      " [5666.06490705 5620.50029203 5574.935677   ... 2706.51006321\n",
      "  2679.1936084  2651.87715359]\n",
      " [5618.27711947 5574.04415856 5529.81119765 ... 2726.59794557\n",
      "  2698.48969642 2670.38144727]\n",
      " [5570.4893319  5527.5880251  5484.68671831 ... 2746.68582794\n",
      "  2717.78578445 2688.88574096]]\n"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.ndimage\n",
    "\n",
    "data_by_filename = {}\n",
    "# Define the bounds for latitude and longitude\n",
    "bounds_lat = [20, 40]\n",
    "bounds_lon = [20, 50]\n",
    "\n",
    "# Define the paths where the NetCDF files are saved\n",
    "files = {\n",
    "    'MSLP_ERA5_October.nc': 'msl',  # 'msl' is the variable name for Mean Sea Level Pressure\n",
    "    'GPH_ERA5_October.nc': 'z'      # 'z' is the variable name for Geopotential Height\n",
    "}\n",
    "\n",
    "# Function to find the indices that match the latitude and longitude bounds\n",
    "def find_lat_lon_indices(latitude, longitude, bounds_lat, bounds_lon):\n",
    "    lat_indices = np.where((latitude >= bounds_lat[0]) & (latitude <= bounds_lat[1]))[0]\n",
    "    lon_indices = np.where((longitude >= bounds_lon[0]) & (longitude <= bounds_lon[1]))[0]\n",
    "    return lat_indices, lon_indices\n",
    "\n",
    "# Function to interpolate the data to target dimensions\n",
    "def extrapolate_to_target_dim(data, target_shape):\n",
    "    zoom_factors = [target_shape[i] / data.shape[i] for i in range(len(target_shape))]\n",
    "    return scipy.ndimage.zoom(data, zoom_factors, order=1)\n",
    "\n",
    "# Loop through the files and process each one\n",
    "for file_name, variable in files.items():\n",
    "    file_path = os.path.join(ERA5_path, file_name)\n",
    "    \n",
    "    try:\n",
    "        # Load the NetCDF file using netCDF4\n",
    "        dataset = nc.Dataset(file_path, mode='r')\n",
    "\n",
    "        # Access the variables\n",
    "        longitude = dataset.variables['longitude'][:]\n",
    "        latitude = dataset.variables['latitude'][:]\n",
    "        time = dataset.variables['time'][:]  # Time steps\n",
    "        data = dataset.variables[variable][:]   # Get the relevant variable (msl or z)\n",
    "\n",
    "        # Find the indices for the lat/lon bounds\n",
    "        lat_indices, lon_indices = find_lat_lon_indices(latitude, longitude, bounds_lat, bounds_lon)\n",
    "\n",
    "        # Subset the data for the defined bounds\n",
    "        data_subset = data[:, lat_indices, :][:, :, lon_indices]  # Subsetting for lat/lon\n",
    "\n",
    "        # Interpolate the subsetted data to the target shape (400, 600)\n",
    "        target_shape = (data_subset.shape[0], 400, 600)  # Keep time, change lat/lon\n",
    "        data_interpolated = extrapolate_to_target_dim(data_subset, target_shape)\n",
    "        data_by_filename[file_name] = data_interpolated\n",
    "        # Check the shape of the interpolated data\n",
    "        print(f\"{file_name} shape for subset (time, lat, lon): {data_interpolated.shape}\")\n",
    "        \n",
    "        # Optional: Print a specific time step for the subsetted region\n",
    "        print(f\"{file_name} data for first time step: {data_interpolated[0, :, :]}\")\n",
    "\n",
    "        # Close the dataset after extracting data\n",
    "        dataset.close()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found at: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chirps_flat():\n",
    "    chirps_dataset = nc.Dataset(f\"{CHIRPS_path}/chirps_octobers_middle_east_1981_2010.nc\") # CHIRPS_Monthly_precipitation\n",
    "    chirps_precip_data = np.array(chirps_dataset.variables['precip'][:])\n",
    "    chirps_2D = np.mean(chirps_precip_data,axis=0)\n",
    "    flat_chirps_2D= chirps_2D.reshape(-1)\n",
    "    return flat_chirps_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precipitation_models() -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Retrieves and processes precipitation models from a specified directory.\n",
    "    Returns:\n",
    "        list[np.ndarray]: A list of NumPy arrays where each array represents the mean\n",
    "                          precipitation data computed across the first axis of the input arrays.\n",
    "    \"\"\"\n",
    "    models: list[np.ndarray] = []\n",
    "    for file in os.listdir(interpulated_cmip_path):\n",
    "        model_path = os.path.join(interpulated_cmip_path, file)\n",
    "        model_data = np.load(model_path)\n",
    "        model_mean_data = np.mean(model_data, axis=0)\n",
    "        models.append(model_mean_data)\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatening the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghp_flat_averaged = np.mean(data_by_filename['GPH_ERA5_October.nc'],axis=0) \n",
    "ghp_flat = ghp_flat_averaged.reshape(-1)\n",
    "\n",
    "mslp_flat_averaged = np.mean(data_by_filename['MSLP_ERA5_October.nc'],axis=0) \n",
    "mslp_flat = mslp_flat_averaged.reshape(-1)\n",
    "\n",
    "y_chirps : np.ndarray = get_chirps_flat()\n",
    "y_chirps = y_chirps.astype(np.float64)\n",
    "\n",
    "# Flattening the data \n",
    "# X : np.ndarray = matrix with the features \n",
    "# sea-level pressure | temp | geo-potetntial pressure \n",
    "# Predicted preciptations (the observed ones)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split The Data randomly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (192000, 2), X_test shape: (48000, 2)\n",
      "y_train shape: (192000,), y_test shape: (48000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `ghp_flat` and `mslp_flat` are already calculated and reshaped to 1D arrays\n",
    "# Combine `ghp_flat` and `mslp_flat` to form X (features)\n",
    "X = np.column_stack((ghp_flat, mslp_flat))\n",
    "\n",
    "# Assuming `y_chirps` is the target variable and already in the correct format\n",
    "# Verify that the lengths match before proceeding\n",
    "assert len(X) == len(y_chirps), \"The lengths of X and y_chirps do not match!\"\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test by default)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_chirps, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the splits\n",
    "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig Linear Regression models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 1.406319733890696e+73\n",
      "Linear Regression R²: 0.010306664613478622\n",
      "Linear Regression Coefficients: [-7.44006046e+31  1.07062795e+33]\n",
      "Linear Regression Intercept: -1.0655355112613028e+38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Create a Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "\n",
    "# Calculate MSE and R² on test data\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Linear Regression MSE: {mse_linear}\")\n",
    "print(f\"Linear Regression R²: {r2_linear}\")\n",
    "print(f\"Linear Regression Coefficients: {linear_model.coef_}\")\n",
    "print(f\"Linear Regression Intercept: {linear_model.intercept_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression (degree 2) MSE: 1.2699815282289448e+73\n",
      "Polynomial Regression (degree 2) R²: 0.1062542718682592\n",
      "Polynomial Regression Coefficients: [ 0.00000000e+00 -1.37708579e+35  3.53940900e+36 -3.74530326e+28\n",
      "  1.36000399e+30 -1.74593213e+31]\n",
      "Polynomial Regression Intercept: -1.793773360681875e+41\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Create polynomial features (degree=2 for quadratic, change degree for higher-order polynomials)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Transform the features to polynomial features\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Create a Linear Regression model (for polynomial regression)\n",
    "poly_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the polynomial features of the training data\n",
    "poly_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predict on the polynomial features of the test data\n",
    "y_pred_poly = poly_model.predict(X_test_poly)\n",
    "\n",
    "# Calculate MSE and R² on test data\n",
    "mse_poly = mean_squared_error(y_test, y_pred_poly)\n",
    "r2_poly = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Polynomial Regression (degree 2) MSE: {mse_poly}\")\n",
    "print(f\"Polynomial Regression (degree 2) R²: {r2_poly}\")\n",
    "print(f\"Polynomial Regression Coefficients: {poly_model.coef_}\")\n",
    "print(f\"Polynomial Regression Intercept: {poly_model.intercept_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coefficient evaluetion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coefficients(coefficients: np.ndarray, model_names: List[str], title: str):\n",
    "    \"\"\"\n",
    "    Plot the coefficients of the regression model.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    coefficients : np.ndarray\n",
    "        The regression coefficients for each model.\n",
    "        \n",
    "    model_names : List[str]\n",
    "        The names or labels of the models corresponding to the coefficients.\n",
    "\n",
    "    title : str\n",
    "        Title of the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(model_names, coefficients, color='skyblue')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dslab",
   "language": "python",
   "name": "dslab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
